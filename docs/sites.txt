
:toc:

Turbine Sites Guide
===================
Justin M. Wozniak <wozniak@mcs.anl.gov>
v0.4.0, August 2013

////

This file is on the web at:

http://www.mcs.anl.gov/exm/local/guides/turbine-sites.html

////

////
Settings:
////
:miscellaneous.newline: \n

== How to use this guide

This manual provides a reference on how to run Swift/T Turbine
programs on a variety of systems.  It also contains an index of sites
maintained by the Swift/T team for use by Turbine.

For each machine, a *public installation* and/or a *build procedure* will
be provided.  The user need only follow one set of directions.

[[login_node]]
A *login node installation* may be available on certain systems.  This
will run Swift/T on the login node of that system.  This only acceptable for
short debugging runs of 1 minute or less.  It will affect other users
so please be cautious when using this mode for debugging.

=== Public installations

These are maintained by the Swift/T team.  Because they may become out
of date after a release, the release version and a timestamp are
recorded below.

To request maintenance on a public installation, simply email
exm-user@mcs.anl.gov .

=== Build procedures

The build procedure is based on the installation process
described in the link:./swift.html[Swift/T Guide].  You
should follow that build procedure, and use this
guide for information on specific configuration
settings for your system.

The settings are generally implemented by modifying the
+exm-settings.sh+ configuration script.  In some cases,
where the setting is not configurable through +exm-settings.sh+,
it may be necessary to directly modify the +configure+ or +make+
command lines by following the manual build process, or by modifying
build scripts under the +build+ subdirectory
(e.g. +turbine-build.sh+).

=== Version numbers

The component version numbers that correspond together to make up a
Swift/T release may be found on the
link:../downloads/downloads.html[Downloads] page.

=== Freshness

These instructions may become stale for various reasons.  For example,
system administrators may update directory locations, breaking these
instructions.  Thus, we mark *As of:* dates on the instructions for
each system.

To report a problem, simply email exm-user@mcs.anl.gov .

=== For more information

* See the link:./swift.html[Swift/T Guide] for more information about Swift/T.
* Join the ExM (Swift/T project) http://lists.mcs.anl.gov/mailman/listinfo/exm-user[user mailing list].

== Turbine as MPI program

Turbine is a moderately complex MPI program.  It is essentially a Tcl
library that glues together multiple C-based systems, including MPI,
ADLB, and the Turbine dataflow library.

Running Turbine on a MPI-enabled system works as follows:

* Compilation and installation: This builds the Turbine libraries and
  links with the system-specific MPI library.  STC must also be
  informed of the Turbine installation to access correct built-in
  function information
* Run-time configuration: The startup job submission script locates
  the Turbine installation and reads configuration information
* Process launch: The Tcl shell, +tclsh+, is launched in parallel and
  configuration information is passed to it so it can find the
  libraries. The Tcl program script is the STC-generated user program
  file.  The MPI library enables communication among the +tclsh+
  processes.

Each of the systems below follows this basic outline.

On simpler systems, use the +turbine+ program.  This is a small shell
script wrapper that configures Turbine and essentially runs:
----
mpiexec tclsh program.tcl
----

On more complex, scheduled systems, users do not invoke +mpiexec+
directly.  Thus, sample scripts are provided below.

[[scheduled_systems]]
== Scheduled systems

On scheduled systems (PBS, SLURM, Cobalt, etc.), Turbine is launched
with a customized _run script_ (+turbine-<name>-run+) that launches Turbine
on that system.  This produces a batch script if necessary and submits
it with the job submission program (e.g., +qsub+).  The working
directory (+PWD+) for the job is called +TURBINE_OUTPUT+; the user may
select this or Turbine will pick one based on the date and report it.
The user program will be copied to +TURBINE_OUTPUT+ before submission.
Standard output and error goes to +TURBINE_OUTPUT/output.txt+.

=== Turbine run scripts

Cobalt:: +turbine-cobalt-run.zsh+
Cray/APRUN:: +turbine-aprun.zsh+ (PBS with Cray's +aprun+)
PBS:: +setup-turbine-pbs.zsh+
SLURM:: +turbine-slurm-run.zsh+

Each script accepts input via environment variables and command-line options.

[[variables]]
=== Turbine scheduler variables

For scheduled systems, Turbine accepts a common set of environment
variables.

+NODES+:: Number of nodes to use
+PPN+:: Number of processes per node
+PROJECT+:: The project name to use with the system scheduler
+TURBINE_OUTPUT+:: Directory in which to place Turbine output
+QUEUE+:: Name of queue in which to run (may leave unset)

[[options]]
=== Turbine run script options

For scheduled systems, Turbine accepts a common set of command line options.

+-d <directory>+:: Set the Turbine output directory (optional, a
default is provided based on the date).

+-e <key>=<value>+:: Set an environment variable in the job
environment.  This may be used multiple times

+-n <procs>+:: Number of processes.

+-o <directory>+:: Set the Turbine output directory root, in which
default Turbine output directories are automatically created based on
the date.

+-s <file>+:: Source this file for environment variables.  These
variables override any other <<variables,Turbine scheduler
variables>>.  You may place arbitrary shell code in this script.

+-t <time>+:: Set scheduler walltime.  The argument format is passed
through to the scheduler

+-V+:: Make script verbose.  This typically just applies +set -x+,
allowing you to inspect variables and arguments as passed to the
system scheduler.

== x86 clusters

[[generic_clusters]]
=== Generic clusters

This is the simplest method to run Turbine.

==== Build procedure

The +exm-setup.zsh+ script should just work.

To run, simply build a MPI hosts file and pass that to Turbine, which
will pass it to +mpiexec+.

----
turbine -l -n 3 -f hosts.txt program.tcl
----

=== MCS compute servers

Compute servers at MCS Division, ANL.
Operates as a generic cluster (see above).

----
echo crush.mcs.anl.gov >  hosts.txt
echo crank.mcs.anl.gov >> hosts.txt
turbine -l -n 3 -f hosts.txt program.tcl
----

==== Public installation

*As of:* trunk, 8/13/2013

MCS users are welcome to use this installation.

* STC: +~wozniak/Public/stc/bin/stc+
* Turbine: +~wozniak/Public/turbine/bin/turbine+

=== Breadboard

Cf. link:http://wiki.mcs.anl.gov/radix/index.php/Breadboard[Breadboard Wiki]

Breadboard is a cloud-ish cluster for software development in
MCS. This is a fragile resource used by many MCS developers. Do not
overuse.

Operates as a generic cluster (see above).  No scheduler.  Once you
have the nodes, you can use them until you release them or time
expires (12 hours by default).

. Allocate nodes with +heckle+.  See Breadboard wiki
. Wait for nodes to boot
. Use +heckle allocate -w+ for better interaction
. Create MPICH hosts file:
+
----
heckle stat | grep $USER | cut -f 1 -d ' ' > hosts.txt
----
. Run:
+
----
export TURBINE_LAUNCH_OPTS='-f hosts.txt'
turbine -l -n 4 program.tcl
----
+
. Run as many jobs as desired on the allocation
. When done, release the allocation:
+
----
for h in $( cat hosts.txt )
do
  heckle free $h
done
----

=== Midway

Midway is a mid-sized SLURM cluster at the University of Chicago

==== Public installation

*As of:* 0.2.1 - 02/11/2013

* STC: +~wozniak/Public/stc-0.0.3/bin/stc+

To run:

----
srun ~wozniak/Public/turbine-0.1.1/scripts/submit/slurm/turbine-slurm.sh -n 3 ~/program.tcl
----

==== Build procedure

* Midway uses OpenMPI.  We have tested with +/software/openmpi-1.6-el6-x86_64+
* Put +mpicc+ in your +PATH+
* Configure ADLB with:
+
----
CC=mpicc LDFLAGS="-Wl,-rpath -Wl,/software/openmpi-1.6-el6-x86_64/lib" --enable-mpi-2
----
* Configure Turbine with:
+
----
 --with-mpi-lib-name=mpi
----

=== Eureka

Eureka is a 100-node x86 cluster at the Argonne Leadership Computing
Facility (ALCF).  It uses the Cobalt scheduler.

==== Build procedure

* Configure Tcl and c-utils with gcc
* Configure ADLB with mpicc

To run:
----
export MODE=cluster
submit/cobalt/turbine-cobalt-run.zsh -n 3 ~/program.tcl
----

The normal Turbine environment variables are honored, plus the
<<variables,Turbine scheduler variables>>.

=== Tukey

https://www.alcf.anl.gov/user-guides/tukey[Tukey] is a 96-node x86
cluster at the Argonne Leadership Computing Facility (ALCF).  It uses
the Cobalt scheduler.

*As of:* Trunk, 7/30/2013

==== Public installation

* STC: +~wozniak/Public/stc-tukey/bin/stc+

To run:
----
export MODE=cluster
export QUEUE=pubnet
export PROJECT=...
~wozniak/Public/turbine-tukey/scripts/submit/cobalt/turbine-cobalt-run.zsh -n 3 program.tcl
----

==== Build procedure

* Check that the system-provided MVAPICH +mpicc+ is in your +PATH+
* Configure c-utils with +gcc+
* Configure ADLB with +CC=mpicc --enable-mpi-2+
* Configure Turbine with +--with-launcher=/soft/libraries/mpi/mvapich2/gcc/bin/mpiexec+

=== Blues

http://www.lcrc.anl.gov/about/blues[Blues] is a 310-node x86
cluster at ANL.  It uses PBS.

==== Public installation

* STC: +~wozniak/Public/stc/bin/stc+

To run:
----
export QUEUE=batch
~wozniak/Public/turbine/scripts/submit/pbs/turbine-pbs-run.zsh -n 3 program.tcl
----

See the <<variables,Turbine scheduler variables>> and
<<options,Turbine run script options>> for additional settings.

==== Build procedure

Apparently need to append to use GCC 4.7.2 and set +LD_LIBRARY_PATH+:
----
$ which gcc
/software/gcc-4.7.2/bin/gcc
$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/software/gcc-4.7.2/lib64
----

== Blue Gene

The Blue Gene systems at ANL are <<scheduled_systems,scheduled
systems>> that use Cobalt.

* The job ID is placed in +TURBINE_OUTPUT/jobid.txt+
* Job metadata is placed in +TURBINE_OUTPUT/turbine-cobalt.log+
* The Cobalt log is placed in +TURBINE_OUTPUT+

=== Blue Gene/P

==== Surveyor/Intrepid/Challenger

These machines are at the Argonne Leadership Computing Facility (ALCF).

===== Public installation

* Based on trunk
* STC: +~wozniak/Public/stc-trunk/bin/stc+

To run:

----
~wozniak/Public/turbine/scripts/submit/cobalt/turbine-cobalt-run.zsh -n 3 ~/program.tcl
----

===== Build procedure

To run on the <<login_node,login node>>:

* Install MPICH for the login nodes
* Configure Tcl and c-utils with gcc
* Configure ADLB with your MPICH
* Configure Turbine with
+
----
--enable-bgp LDFLAGS=-shared-libgcc
----
+
This makes adjustments for some Blue Gene quirks.

* Then, simply use the +bin/turbine+ program to run.  Be cautious in
  your use of the login nodes to avoid affecting other users.

_To run on the compute nodes under IBM CNK:_

In this mode, you cannot use +app+ functions to launch external
programs because CNK does not support this.  See ZeptoOS below.

* Configure Tcl with mpixlc
* Configure c-utils with gcc
* Configure ADLB with:
+
----
--enable-xlc
CC=/bgsys/drivers/ppcfloor/comm/bin/mpixlc
----
* Configure Turbine with:
+
----
CC=/soft/apps/gcc-4.3.2/gnu-linux/bin/powerpc-bgp-linux-gcc
--enable-custom
--with-mpi-include=/bgsys/drivers/V1R4M2_200_2010-100508P/ppc/comm/default/include
----

To run, use +scripts/submit/bgp/turbine-cobalt.zsh+
See the script header for usage.

_To run on the compute nodes under ZeptoOS:_

* Configure Tcl with zmpicc
* Configure c-utils with gcc
* Configure ADLB with
+
----
CC=zmpicc --enable-mpi-2
----
* Configure Turbine with
+
----
CC=/soft/apps/gcc-4.3.2/gnu-linux/bin/powerpc-bgp-linux-gcc
--enable-custom
--with-mpi-include=/bgsys/drivers/V1R4M2_200_2010-100508P/ppc/comm/default/include
----

To run, use +scripts/submit/bgp/turbine-cobalt.zsh+
See the script header for usage.

=== Blue Gene/Q

*As of:* 0.4.0 - 07/29/2013

==== Vesta

===== Public installation

* STC: +~wozniak/Public/stc-bgq/bin/stc+
* Turbine: +~wozniak/Public/turbine-bgq/scrips/submit/cobalt/turbine-cobalt-run.zsh+
* Run as:
+
----
export MODE=BGQ
export QUEUE=<queue_name>
turbine-cobalt-run.zsh -n 3 program.tcl
----

The normal Turbine environment variables are honored, plus the
<<variables,Turbine scheduler variables>>.

===== Build procedure

*Tcl:*

The GCC installation does not support shared libraries.  Thus, you
must compile Tcl with +bgxlc+.  You must modify the Makefile to use
+bgxlc+ arguments: +-qpic+, +-qmkshrobj+.  You must link with
+-qnostaticlink+.

You may get errors that say +wrong digit+.  This is apparently a bgxlc
bug when applied to Tcl's +StrToD.c+.  Compiling this file with +-O3+ fixes
the problem.

Put +/bgsys/drivers/ppcfloor/comm/gcc/bin+ in your +PATH+.

* Compile c-utils with gcc
* Configure ADLB with +mpicc+ and +--enable-mpi-2+
* Configure Turbine

*External scripting:*

* Python
** Configure Python with BGXLC
* R
** Configure R with GCC as usual
** Run with:
+
----
turbine-cobalt-run.zsh -e R_HOME=/path/to/R/lib64/R -e LD_LIBRARY_PATH=/path/to/R/lib64/R/lib
----

== Cray

=== Blue Waters

Blue Waters is a Cray XE6/XK7 at the University of Illinois at
Urbana-Champaign.

==== Build procedure

*As of:*  11/05/2013

Cray systems do not use +mpicc+.  We set +CC=gcc+ and use compiler
flags to configure the MPI library.

* Configure ADLB with:
+
----
./configure --prefix=/path/to/lb --with-c-utils=/path/to/c-utils
CC=gcc
CFLAGS=-I/opt/cray/mpt/default/gni/mpich2-gnu/47/include
LDFLAGS="-L/opt/cray/mpt/default/gni/mpich2-gnu/47/lib -lmpich"
--enable-mpi-2
----
+
* In the Turbine configure step, replace the +--with-mpi+ option with:
+
----
--enable-custom-mpi --with-mpi=/opt/cray/mpt/default/gni/mpich2-gnu/47
----

==== Submitting jobs
Submitting jobs on Blue Waters is largely the same with with other Cray
systems. One difference is that the size of the job is specified using a
different notation.

Blue Waters requires the submit script to specify job size using
different directives to other Cray systems.  It does *not* support the
'mpp' directives: trying to use an 'mpp' directive may cause your job
to be rejected or stuck in the queue. The correct directive is:
----
#PBS -l nodes=1:ppn=32
----

The +turbine-aprun-run.zsh+ script supports Blue Waters.  You can invoke
it as follows (for a single node/32 processes per node):
----
QUEUE=normal BLUE_WATERS=true PPN=32 turbine-aprun-run.zsh -n 32 helloworld.tcl
----


==== Installing from source
===== Prerequisites
* Tcl 8.5 installed: /usr/bin/tclsh
* Swig is preinstalled: /usr/bin/swig (SWIG Version 1.3.36)
* Now we need to ensure that the right modules are loaded.
* Switch programming env to use gcc.
----
module unload PrgEnv-cray
module load PrgEnv-gnu
----
* Load module with latest Oracle Java JDK 7+
----
module load java
----
* Download and install the Apache Ant build tool (required for STC)
----
wget http://www.apache.org/dist/ant/binaries/apache-ant-1.9.2-bin.tar.bz2
# Check archive is valid
ant_xsum=$(shasum apache-ant-1.9.2-bin.tar.bz2 | awk '{ print $1 }')
if [ ! "$ant_xsum" = "50cfaaeecee4f88a3ff9de5068fc98e4e9268daf" ]
then
  echo "Bad ant download checksum"
fi

# Extract ant install ant somewhere permanent
tar xvjf apache-ant-1.9.2-bin.tar.bz2
mkdir -p ~/soft
mv apache-ant-1.9.2/ ~/soft/

# Add ant to path (put this in .bashrc)
export PATH="$PATH:$HOME/soft/apache-ant-1.9.2/bin"

# Check ant version
ant -version
----

===== Installation
- Need to install to a lustre fs:
  - /scratch (not backed up, best performance)
  - /u home directory (backed up, good performance)
  - /projects (backed up, good performance)

- I used a prepackaged distro built using "distro/construct.zsh -t" to build
  from trunk. The following instructions are to install from this distro to
  trunk on Blue Waters.
- First extract the tarball

----
tar xvzf exm-trunk.tar.gz
cd exm-trunk
----

- exm-settings.zsh needs some customization.  The changed settings were:
----
EXM_PREFIX=/u/sciteam/tarmstro/soft/exm-trunk-r8770

# Use the latest GNU-compatible version of mpich
EXM_MPI=/opt/cray/mpt/default/gni/mpich2-gnu/48
# Need to use gcc (mpicc doesn't exist on Cray)
EXM_MPICC=`which gcc`

# Custom MPI
EXM_CUSTOM_MPI=1

# Since we're not using mpicc wrapper, add CC options for MPI libraries
export CFLAGS="-I/opt/cray/mpt/default/gni/mpich2-gnu/48/include/"
export LDFLAGS="-L/opt/cray/mpt/default/gni/mpich2-gnu/48/lib/ -lmpich"

# Currently MPI 3 not supported
MPI_VERSION=2
----
- I also needed to make a change to the configure script (as of Aug 29 2013)
  so that it found the tcl shared library in /usr/lib64 instead of /usr/lib
- After that the +exm-setup.zsh+ script should take care of installation
----
./exm-setup.zsh
----


=== Beagle

Beagle is a Cray XE6 at the University of Chicago

Remember that at run time, Beagle jobs can access only +/lustre+, not
NFS (including home directories).  Thus, you must install Turbine and
its libraries in +/lustre+.  Also, your data must be in +/lustre+.

==== Public installation

===== Login nodes

This installation is for use on the <<login_node,login node>>.

* Swift/T trunk - 6/11/2013
* Turbine: +~wozniak/Public/turbine-trunk-beagle-login+
* STC: +~wozniak/Public/stc-trunk-beagle-login+

===== Compute nodes

* Swift/T trunk - 6/11/2013
* Turbine: +/lustre/beagle/wozniak/Public/turbine+
* STC: +/lustre/beagle/wozniak/Public/stc+

To run:

1. Set environment variables.  The normal Turbine environment
variables are honored, plus the <<variables,Turbine scheduler
variables>>.
2. Run submit script (in +turbine/scripts/submit/cray+):
+
----
turbine-aprun-run.zsh script.tcl --arg1=value1 ...
----

==== Build procedure

Cray systems do not use +mpicc+.  We set +CC=gcc+ and use compiler
flags to configure the MPI library.

* Configure ADLB with:
+
----
./configure --prefix=/path/to/lb --with-c-utils=/path/to/c-utils
CC=gcc
CFLAGS=-I/opt/cray/mpt/default/gni/mpich2-gnu/47/include
LDFLAGS="-L/opt/cray/mpt/default/gni/mpich2-gnu/47/lib -lmpich"
--enable-mpi-2
----
+
* In the Turbine configure step, replace the +--with-mpi+ option with:
+
----
--enable-custom-mpi --with-mpi=/opt/cray/mpt/default/gni/mpich2-gnu/47
----

==== Build procedure with MPE

Configure MPE 1.3.0 with:
----
export CFLAGS=-fPIC
export MPI_CFLAGS="-I/opt/cray/mpt/default/gni/mpich2-gnu/47/include -fPIC"
export LDFLAGS="-L/opt/cray/mpt/default/gni/mpich2-gnu/47/lib -lmpich"
export F77=gfortran
export MPI_F77=$F77
export MPI_FFLAGS=$MPI_CFLAGS
CC="gcc -fPIC" ./configure --prefix=... --disable-graphics
----

Configure ADLB with:
----
export CFLAGS=-mpilog
export LDFLAGS="-L/path/to/mpe/lib -lmpe -Wl,-rpath -Wl,/path/to/mpe/lib"
./configure --prefix=... CC=mpecc --with-c-utils=/path/to/c-utils --with-mpe=/path/to/mpe --enable-mpi-2
----

Configure Turbine with:
----
./configure --enable-custom-mpi --with-mpi=/opt/cray/mpt/default/gni/mpich2-gnu/47 --with-mpe=/path/to/mpe
----

=== Raven

Raven is a Cray XE6/XK7 at Cray.

==== Build procedure

* Configure ADLB with:
+
----
./configure --prefix=/path/to/lb --with-c-utils=/path/to/c-utils
CC=gcc
CFLAGS=-I/opt/cray/mpt/default/gni/mpich2-gnu/46/include
LDFLAGS="-L/opt/cray/mpt/default/gni/mpich2-gnu/46/lib -lmpich"
--enable-mpi-2
----
+
* In the Turbine configure step, use:
+
----
--with-mpi=/opt/cray/mpt/default/gni/mpich2-gnu/46
----
* Use this Java when compiling/running STC: +/opt/java/jdk1.7.0_07/bin/java+

To run:

1. Set environment variables.  The normal Turbine environment
variables are honored, plus the <<variables,Turbine scheduler
variables>>.
2. Run submit script (in +turbine/scripts/submit/cray+):
+
----
turbine-aprun-run.zsh script.tcl --arg1=value1 ...
----

==== Advanced usage:

Turbine uses a PBS template file called
+turbine/scripts/submit/cray/turbine-aprun.sh.m4+.  This file is
simply filtered and submitted via +qsub+.  You can edit this file to
add additional settings as necessary.

==== Module:

You may load Swift/T with:

----
module use /home/users/p01577/Public/modules
module load swift-t
----

== Cloud

=== EC2

==== Setup

* Install http://instagram-engineering.tumblr.com/post/11399488246/simplifying-ec2-ssh-connections[+ec2-host+] on your local system
* Launch EC2 instances.
** Enable SSH among instances.
** Firewall settings must allow all all TCP/IP traffic for MPICH to run.
** If necessary, install Swift/T
** An AMI with Swift/T installed is available
* Use the provided script +turbine/scripts/submit/ec2/turbine-setup-ec2.zsh+.
** See the script header for usage notes
** This will configure SSH settings and create a hosts file for MPICH
   and install them on the EC2 instance

To run:
----
export TURBINE_LAUNCH_OPTS="-f $HOME/hosts.txt"
turbine program.tcl
----

////
Local Variables:
mode: doc
eval: (auto-fill-mode 1)
End:
////
